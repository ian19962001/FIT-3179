{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6947891",
   "metadata": {},
   "source": [
    "# Statistical Machine Learning (F20ML) Coursework 2022-2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b418409",
   "metadata": {},
   "source": [
    "### Contents included in this Python source code:\n",
    "- Information and breakdown of the dataset\n",
    "- Decision Tree algorithm for classification\n",
    "- Alternative approach to classify the dataset (s.t. Neural Network, Na√Øve Bayes and KNN)\n",
    "- Finetune, cross-validate and perform evaluation for the classification algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293235b3",
   "metadata": {},
   "source": [
    "Import basic tools to compute the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0390b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cda5d2",
   "metadata": {},
   "source": [
    "Import data provided from the source below:\n",
    "\n",
    "**Data Source:** https://archive.ics.uci.edu/ml/datasets/wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8af25f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"wine.data\", names = [\"Class\",\n",
    "                                       \"Alcohol\",\n",
    "                                       \"Malic acid\",\n",
    "                                       \"Ash\",\n",
    "                                       \"Alcalinity of ash\",\n",
    "                                       \"Magnesium\",\n",
    "                                       \"Total phenols\",\n",
    "                                       \"Flavanoids\",\n",
    "                                       \"Nonflavanoid phenols\",\n",
    "                                       \"Proanthocyanins\",\n",
    "                                       \"Color intensity\",\n",
    "                                       \"Hue\",\n",
    "                                       \"OD280/OD315 of diluted wines\",\n",
    "                                       \"Proline\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dfb3a9",
   "metadata": {},
   "source": [
    "Identify missing/invalid value and type of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "906f9b75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Class                         178 non-null    int64  \n",
      " 1   Alcohol                       178 non-null    float64\n",
      " 2   Malic acid                    178 non-null    float64\n",
      " 3   Ash                           178 non-null    float64\n",
      " 4   Alcalinity of ash             178 non-null    float64\n",
      " 5   Magnesium                     178 non-null    int64  \n",
      " 6   Total phenols                 178 non-null    float64\n",
      " 7   Flavanoids                    178 non-null    float64\n",
      " 8   Nonflavanoid phenols          178 non-null    float64\n",
      " 9   Proanthocyanins               178 non-null    float64\n",
      " 10  Color intensity               178 non-null    float64\n",
      " 11  Hue                           178 non-null    float64\n",
      " 12  OD280/OD315 of diluted wines  178 non-null    float64\n",
      " 13  Proline                       178 non-null    int64  \n",
      "dtypes: float64(11), int64(3)\n",
      "memory usage: 19.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info() # check data type and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3782277d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>14.39</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.45</td>\n",
       "      <td>14.6</td>\n",
       "      <td>96</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.02</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>14.06</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.61</td>\n",
       "      <td>17.6</td>\n",
       "      <td>121</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.05</td>\n",
       "      <td>1.06</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>13.86</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.27</td>\n",
       "      <td>16.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.85</td>\n",
       "      <td>7.22</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0      1    14.23        1.71  2.43               15.6        127   \n",
       "1      1    13.20        1.78  2.14               11.2        100   \n",
       "2      1    13.16        2.36  2.67               18.6        101   \n",
       "3      1    14.37        1.95  2.50               16.8        113   \n",
       "4      1    13.24        2.59  2.87               21.0        118   \n",
       "5      1    14.20        1.76  2.45               15.2        112   \n",
       "6      1    14.39        1.87  2.45               14.6         96   \n",
       "7      1    14.06        2.15  2.61               17.6        121   \n",
       "8      1    14.83        1.64  2.17               14.0         97   \n",
       "9      1    13.86        1.35  2.27               16.0         98   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "5           3.27        3.39                  0.34             1.97   \n",
       "6           2.50        2.52                  0.30             1.98   \n",
       "7           2.60        2.51                  0.31             1.25   \n",
       "8           2.80        2.98                  0.29             1.98   \n",
       "9           2.98        3.15                  0.22             1.85   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  \n",
       "5             6.75  1.05                          2.85     1450  \n",
       "6             5.25  1.02                          3.58     1290  \n",
       "7             5.05  1.06                          3.58     1295  \n",
       "8             5.20  1.08                          2.85     1045  \n",
       "9             7.22  1.01                          3.55     1045  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just display to the first 10 rows.\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c0e842",
   "metadata": {},
   "source": [
    "Organise and split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f841de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8e4f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the values for the x an y where x is without class \n",
    "#y takes all values of class\n",
    "X = df.drop(\"Class\", axis = 1).values # putting feature variable to X\n",
    "Y = df[\"Class\"].values # putting response variable to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abd66541",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,random_state=8849) # split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046bb1e",
   "metadata": {},
   "source": [
    "## Decision Tree Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2ce2b",
   "metadata": {},
   "source": [
    "Implementing **Decision Tree Algorithm** for classification from scratch:\n",
    "\n",
    "**Code Ref. 1:** https://github.com/AssemblyAI-Examples/Machine-Learning-From-Scratch/blob/main/04%20Decision%20Trees/DecisionTree.py\n",
    "\n",
    "**Code Ref. 2:** http://www.oranlooney.com/post/ml-from-scratch-part-4-decision-tree/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a06df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3b3c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Node class \n",
    "    \"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,value=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f27a1178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeGraph:\n",
    "    \"\"\"\n",
    "    This is a class to create a DecisionTreeGraph. It contains all functions needed for a decision tree.\n",
    "    Object from this class can use any method from this class. \n",
    "\n",
    "    To create the decision tree I referred to the website mentioned below. \n",
    "    @source : https://github.com/AssemblyAI-Examples/Machine-Learning-From-Scratch/blob/main/04%20Decision%20Trees/DecisionTree.py\n",
    "    \"\"\"\n",
    "    def __init__(self, min_split=2, max_depth=20, feature=None):\n",
    "        \"\"\"\n",
    "        @param min_split: this parameter is used to know by how much the samples need to be split. \n",
    "                If a value is not intialized, min_sample will have a value of 2\n",
    "        @param max_depth: this parameter is used to see the maximum depth. \n",
    "                If no value has been initialized, max_depth takes a value of 50. \n",
    "        @param feature: This variable is used to store the feature. \n",
    "                If no feature is intialized then the feature will be None\n",
    "        @param root: The root of the decision tree is none. \n",
    "\n",
    "        init method is used t initialized (constructor) all the variables needed for the decision tree. \n",
    "        \"\"\"\n",
    "        self.min_split=min_split\n",
    "        self.max_depth=max_depth\n",
    "        self.feature=feature\n",
    "        self.root=None\n",
    "\n",
    "\n",
    "    def fit(self, X, y): \n",
    "        \"\"\"\n",
    "        @param feature: This parameter use the numpy function to store the columns\n",
    "        @param X: This parameter takes the X train values. X is independent \n",
    "        @param y: This parameter takes the y train values. y is responsive\n",
    "\n",
    "        This method is used to create a decision tree classifier from the training set X and y\n",
    "        \"\"\"\n",
    "        if not self.feature: # the if condition checks to see if the node has any features. \n",
    "            #If the node has features then we enter\n",
    "            self.feature = X.shape[1]  # we set the features to include the columns \n",
    "        else: #Enter this condition because there is no features, hence we need to start from the root. \n",
    "            self.features = min(X.shape[1],self.feature) #We check the minimum value for the features. \n",
    "        self.root = self.grow_tree(X,y)  # we set the grow tree method from the root.\n",
    "\n",
    "\n",
    "    def grow_tree(self, X, y, current_max_depth=0):\n",
    "        \"\"\"\n",
    "        @param X: This parameter takes the X train values. X is independent \n",
    "        @param y: This parameter takes the y train values. y is responsive\n",
    "        @param current_max_depth: This parameter keeps count of the changes in depth value.\n",
    "        @param noOfLabels: This variable is used to store the length. \n",
    "        This method helps to create a decision tree from a dataset. The growth starts from the root and then\n",
    "        \"\"\"\n",
    "        # Here the .shapereturn the number of elements in the first dimension which is the row and also return the number of elements in the second dimension which is columns \n",
    "        rows, columns = X.shape \n",
    "        # Here np.unique returns the unique values in a list. This can be in any number of nested list. After we get the unique values, we can use the len to find the length\n",
    "        #This length value is stored in a parameter called noOfLabels\n",
    "        noOfLabels = len(np.unique(y)) \n",
    "        \n",
    "        if (current_max_depth>=self.max_depth or noOfLabels==1 or rows<self.min_split):\n",
    "            \n",
    "            counter = Counter(y)\n",
    "            leaf_value = counter.most_common(1)[0][0]\n",
    "            child_node = Node()\n",
    "            child_node.value = leaf_value\n",
    "            return child_node\n",
    "        else:\n",
    "            features_index = np.random.choice(columns, self.feature, replace=False)\n",
    "\n",
    "            # find the best split\n",
    "            best_feature, best_threshold = self.perfect_split(X, y, features_index)\n",
    "\n",
    "            # create child nodes\n",
    "            child_node = Node() #creating a new child node\n",
    "            left_index, right_index = self.split(X[:, best_feature], best_threshold)\n",
    "            child_node.left = self.grow_tree(X[left_index, :], y[left_index], current_max_depth+1)\n",
    "            child_node.right = self.grow_tree(X[right_index, :], y[right_index], current_max_depth+1)\n",
    "            child_node.feature = best_feature\n",
    "            child_node.threshold = best_threshold\n",
    "            return child_node\n",
    "    \n",
    "    def perfect_split(self, X, y, features_index):\n",
    "        best_gain = -float(\"inf\") # to start with the lowest value. \n",
    "        split_index = 0 # Stores the index. Initalised with 0 as that is the start index.\n",
    "        split_threshold = 0 # Stores the threshold. Initalised with 0 as that is the start index.\n",
    "\n",
    "        for i in features_index:\n",
    "            columns = X[:, i] # here it stores the columns in x\n",
    "            thresholds = np.unique(columns) # we can use unique to fin d\n",
    "\n",
    "            for thr in thresholds:\n",
    "                # calculate the information gain\n",
    "                gain = self._information_gain(y, columns, thr)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_index = i\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_index, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        # parent entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # create children\n",
    "        left_index, right_index = self.split(X_column, threshold)\n",
    "\n",
    "        if len(left_index) == 0 or len(right_index) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # calculate the weighted avg. entropy of children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_index), len(right_index)\n",
    "        e_l, e_r = self._entropy(y[left_index]), self._entropy(y[right_index])\n",
    "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
    "\n",
    "        # calculate the IG\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def split(self, X_column, split_thresh):\n",
    "        left_index = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_index = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_index, right_index\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log(p) for p in ps if p>0])\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.tree_traversal(x, self.root) for x in X])\n",
    "\n",
    "    def tree_traversal(self, x,node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self.tree_traversal(x, node.left)\n",
    "        return self.tree_traversal(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2268ad",
   "metadata": {},
   "source": [
    "Build Decision Classification Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb473570",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeGraph()\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413285c3",
   "metadata": {},
   "source": [
    "Test accuracy of the Decision Classification Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b8c9d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4be60af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the Decision Tree is 0.8444444444444444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.91      0.87        11\n",
      "           2       0.87      0.72      0.79        18\n",
      "           3       0.83      0.94      0.88        16\n",
      "\n",
      "    accuracy                           0.84        45\n",
      "   macro avg       0.84      0.86      0.85        45\n",
      "weighted avg       0.85      0.84      0.84        45\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "y_pred = dt.predict(X_test) # to predict the x test values. \n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred) # to calculate the accuracy score\n",
    "report = classification_report(y_test, y_pred) # x test values classification report \n",
    "cm = confusion_matrix(y_test, y_pred) # compute the confusion matrix\n",
    "\n",
    "print(\"The accuracy score of the Decision Tree is\", accuracy); \n",
    "print(report)\n",
    "heatMapPlot = sns.heatmap(cm, cmap =sns.cubehelix_palette(as_cmap=True), annot = True, cbar = False, xticklabels = [\"1\", \"2\", \"3\"], yticklabels = [\"1\", \"2\", \"3\"],linewidth=.5,fmt=\".1f\")\n",
    "heatMapPlot.xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06f84e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Score: 0.8414373918210234\n",
      "Accuracy Score: 0.8444444444444444\n",
      "Precision Score: 0.8466666666666667\n",
      "Recall Score: 0.8444444444444444\n"
     ]
    }
   ],
   "source": [
    "print(\"F Score:\", f1_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\", precision_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"Recall Score:\", recall_score(y_test, y_pred, average ='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb93cf21",
   "metadata": {},
   "source": [
    "Finetune the hyperparameters of Decision Classification Tree while performing 10-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "816b3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random = df.sample(frac = 1)\n",
    "\n",
    "X_random = df_random.drop(\"Class\", axis = 1) # putting randomized feature variable to X\n",
    "y_random = df_random[\"Class\"] # putting randomized response variable to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc2fd69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the data is: 178\n",
      "When we perform 10-fold cross-validation, we get: 17.8 we need to split the value to not get decimal points.\n",
      "After we split the values, we get 17 and 8\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of the data is:\",len(df_random))\n",
    "partition = len(df_random)//10\n",
    "print(\"When we perform 10-fold cross-validation, we get:\",len(df_random)/10, \"we need to split the value to not get decimal points.\"  ) \n",
    "\n",
    "partition_remainder = len(df_random)%10\n",
    "print(\"After we split the values, we get \"+str(partition)+\" and \"+str(partition_remainder))\n",
    "\n",
    "\n",
    "partitionlst = [] #list to store the partitioned values. \n",
    "#as we are doing 10 folds cross validation and we split the data into 10 parts each, the for loop as to range over 10. \n",
    "#As we started from 0 and ends on 9, this comes up to a total of 10. \n",
    "for i in range(0, 10):\n",
    "    if len(partitionlst) == 0:\n",
    "        partitionlst.append(0) # the first value has to be zero. \n",
    "    #this if condition checks if the i value is less than the remainder value(partition_remainder).\n",
    "    # If it is then we enter this condition. \n",
    "    if i+1 <= partition_remainder:\n",
    "        partitionlst.append((i+1) * partition + (i+1))\n",
    "    # If the i value is greater than the remainder value then we enter this condition. \n",
    "    else:\n",
    "        partitionlst.append((i+1) * partition + partition_remainder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e97398a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth : 2  minimum split : 2  accuracy : 0.9444444444444444\n",
      "max_depth : 2  minimum split : 3  accuracy : 0.9444444444444444\n",
      "max_depth : 2  minimum split : 4  accuracy : 0.9444444444444444\n",
      "max_depth : 2  minimum split : 5  accuracy : 0.9444444444444444\n",
      "max_depth : 2  minimum split : 6  accuracy : 0.9444444444444444\n",
      "max_depth : 3  minimum split : 2  accuracy : 0.95679012345679\n",
      "max_depth : 3  minimum split : 3  accuracy : 0.95679012345679\n",
      "max_depth : 3  minimum split : 4  accuracy : 0.95679012345679\n",
      "max_depth : 3  minimum split : 5  accuracy : 0.9629629629629631\n",
      "max_depth : 3  minimum split : 6  accuracy : 0.95679012345679\n",
      "max_depth : 4  minimum split : 2  accuracy : 0.9317356572258534\n",
      "max_depth : 4  minimum split : 3  accuracy : 0.9502541757443718\n",
      "max_depth : 4  minimum split : 4  accuracy : 0.9506172839506173\n",
      "max_depth : 4  minimum split : 5  accuracy : 0.95679012345679\n",
      "max_depth : 4  minimum split : 6  accuracy : 0.95679012345679\n",
      "max_depth : 5  minimum split : 2  accuracy : 0.9379084967320261\n",
      "max_depth : 5  minimum split : 3  accuracy : 0.9444444444444444\n",
      "max_depth : 5  minimum split : 4  accuracy : 0.9564270152505446\n",
      "max_depth : 5  minimum split : 5  accuracy : 0.9444444444444444\n",
      "max_depth : 5  minimum split : 6  accuracy : 0.9629629629629631\n",
      "max_depth : 6  minimum split : 2  accuracy : 0.9197530864197532\n",
      "max_depth : 6  minimum split : 3  accuracy : 0.9382716049382714\n",
      "max_depth : 6  minimum split : 4  accuracy : 0.9564270152505446\n",
      "max_depth : 6  minimum split : 5  accuracy : 0.9629629629629631\n",
      "max_depth : 6  minimum split : 6  accuracy : 0.95679012345679\n",
      "max_depth : 7  minimum split : 2  accuracy : 0.95679012345679\n",
      "max_depth : 7  minimum split : 3  accuracy : 0.9502541757443718\n",
      "max_depth : 7  minimum split : 4  accuracy : 0.9313725490196079\n",
      "max_depth : 7  minimum split : 5  accuracy : 0.9506172839506173\n",
      "max_depth : 7  minimum split : 6  accuracy : 0.9629629629629631\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5) # To get the a the same random value.\n",
    "lst=[] # empty list used for storing the values from each iteration. \n",
    "\n",
    "current_max_depth = 2 # to set the starting depth as 2\n",
    "final_max_depth = 7 # setting the maximum depth as 7\n",
    "#These 2 values mentioned above are used for the condition in the while loop.\n",
    "while current_max_depth<= final_max_depth:\n",
    "    \"\"\"\n",
    "    This while loop checks if the current depth is less than or equal to the final depth.\n",
    "    If the current depth is less than or equal to the final depth, then can enter the loop. \n",
    "    In each iteration, the current_max_depth increments a value of 1. This prevents the loop from running forever.\n",
    "    If the current depth is more than the final depth, do not enter this loop.\n",
    "    \"\"\"\n",
    "    for min_samples_split in range(2, 7):        \n",
    "        accuracy = [] # creating a table to keep atll the accuracy values.\n",
    "        for i in range(1, len(partitionlst[1:])): \n",
    "            \n",
    "            X_train = X_random.drop(X_random.index[range(partitionlst[i-1], partitionlst[i] )]).values\n",
    "            X_test = X_random.iloc[partitionlst[i-1]:partitionlst[i], :].values\n",
    "            y_train = y_random.drop(y_random.index[range(partitionlst[i-1], partitionlst[i])]).values\n",
    "            y_test = y_random.iloc[partitionlst[i-1]:partitionlst[i]].values\n",
    "            \n",
    "            HTCV_dt = DecisionTreeGraph(max_depth = current_max_depth, min_split = min_samples_split)\n",
    "            HTCV_dt.fit(X_train, y_train)\n",
    "            \n",
    "            HTCV_y_pred = HTCV_dt.predict(X_test)\n",
    "            new_acc = accuracy_score(y_test, HTCV_y_pred)\n",
    "            \n",
    "            accuracy.append(new_acc)    \n",
    "        lst.append([current_max_depth,min_samples_split,np.mean(accuracy)])\n",
    "        print(\"max_depth :\", current_max_depth, \" minimum split :\", min_samples_split, \" accuracy :\", np.mean(accuracy))\n",
    "    current_max_depth+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30d924a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum depth is:  3 minimum split is:  5 accuracy is:  0.9629629629629631 \n",
      "\n",
      "maximum depth is:  5 minimum split is:  6 accuracy is:  0.9629629629629631 \n",
      "\n",
      "maximum depth is:  6 minimum split is:  5 accuracy is:  0.9629629629629631 \n",
      "\n",
      "maximum depth is:  7 minimum split is:  6 accuracy is:  0.9629629629629631 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# addded new code to find the best accuracy \n",
    "def max_acc(lst):\n",
    "    \"\"\"\n",
    "    @param lst: this list has the values of the best accuracy\n",
    "    This method helps find the best accuracy from a list of accuracy. \n",
    "    \"\"\"\n",
    "    max_value = [[0,0,0]] #the worst accuracy is 0 and hence we initialise with it. \n",
    "    for i in range(len(lst)): #the for loop is used to loop through the list. Len(lst) is the length of the list\n",
    "        # the if condition has lst[i][2] which gives the accuracy value in the list in the ith iteration.\n",
    "        # we can then use the greater than symbol to check if the max_value accuracy is smaller. If smaller then we enter this condition. \n",
    "        if lst[i][2]>max_value[0][2]: \n",
    "             # This is to reset the max_value as there could be multiple values with the same accuracy. \n",
    "             # Hence if we enter this loop, it means that there is a new max_value. Hence we can reset the old max_value's values. \n",
    "            max_value = [[0,0,0]] \n",
    "            # The new max_value is being stored. \n",
    "            # Only the first dimension of the list is used as we need to store the entire lst[i] to show the maximum depth and minimum depth. \n",
    "            max_value[0] = lst[i] \n",
    "        elif lst[i][2]==max_value[0][2]: #This if condition is used to check if the maximum value and the list in ith iteration have the same values.\n",
    "            # When we enter this condition, it means that there are multiple values with the same current maximum accuracy. \n",
    "            # Hence store all the values. append() is used to insert the value to the max_value\n",
    "            max_value.append(lst[i]) \n",
    "    return max_value # returns the best maximum accuracy value. \n",
    "\n",
    "# calling from the previous run. Taking the output. Then this output is run in the function mac_acc\n",
    "# best_acc will store the value/values with the best accuracy. \n",
    "best_acc = max_acc(lst) \n",
    "# sorting the output to get the best to worst in order. \n",
    "# This means that it sorts the list according to the lowest depth and the lowest split. \n",
    "best_acc.sort() \n",
    "# Using a for loop to print out the values in the best_acc\n",
    "for i in best_acc:\n",
    "    print(\"maximum depth is: \",i[0],\"minimum split is: \",i[1], \"accuracy is: \",i[2],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e14c9294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with the value calculated and since it is sorted. Can the first value in the list. \n",
    "optimized_dt = DecisionTreeGraph(max_depth = best_acc[0][0], min_split = best_acc[0][1])\n",
    "optimized_dt.fit(X_train, y_train) # to fit the train data to the new optimized distribution tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d08d6f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Decision Tree is 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         7\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        17\n",
      "   macro avg       1.00      1.00      1.00        17\n",
      "weighted avg       1.00      1.00      1.00        17\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "#used to predict the new optimized data\n",
    "y_pred = optimized_dt.predict(X_test) \n",
    "\n",
    "accuracy  = accuracy_score(y_test, y_pred) # calculates the accuracy score\n",
    "report = classification_report(y_test, y_pred) # X test classification report\n",
    "cm = confusion_matrix(y_test, y_pred) # to calculate the confusion matrix of x test\n",
    "\n",
    "print(\"The accuracy of the Decision Tree is\", accuracy); \n",
    "print(report)\n",
    "heatMapPlot = sns.heatmap(cm, cmap =sns.cubehelix_palette(as_cmap=True), annot = True, cbar = False, xticklabels = [\"1\", \"2\", \"3\"], yticklabels = [\"1\", \"2\", \"3\"],linewidth=.5,fmt=\".1f\")\n",
    "heatMapPlot.xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d75c41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Score: 1.0\n",
      "Accuracy Score: 1.0\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"F Score:\", f1_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\", precision_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"Recall Score:\", recall_score(y_test, y_pred, average ='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c1ca2",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950dc096",
   "metadata": {},
   "source": [
    "Code source: https://www.youtube.com/watch?v=8zwILUzux6o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9dc128",
   "metadata": {},
   "source": [
    "## Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934537e",
   "metadata": {},
   "source": [
    "Code source: https://www.youtube.com/watch?v=Q93IWdj5Td4\n",
    "\n",
    "https://www.datacamp.com/tutorial/naive-bayes-scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d679e1f",
   "metadata": {},
   "source": [
    "Build Naive Bayes Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "471149ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08095d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3056b2bd",
   "metadata": {},
   "source": [
    "Test accuracy of the Naive Bayes Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29bad3b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ian\\Desktop\\Thomas\\thomasfinalre.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ian/Desktop/Thomas/thomasfinalre.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m evaluation_metrics(X_test, y_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluation_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "evaluation_metrics(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e5d92",
   "metadata": {},
   "source": [
    "Finetune the hyperparameters of Naive Bayes Classifier while applying 10-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "\n",
    "clf_grid_search = GridSearchCV(estimator=clf, param_grid=clf_params, cv=10, verbose=1, scoring='accuracy') \n",
    "clf_grid_search.fit(X_train, y_train)\n",
    "print(clf_grid_search.best_params_)\n",
    "print(clf_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f922cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB(var_smoothing = 6.579332246575683e-06)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4cf5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f18ae99",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e6a34d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b23c961ca40752c0e7cc5aa8b6e7f793ceb428b075191548f50526ee1923447"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
